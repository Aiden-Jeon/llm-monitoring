{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "Setup llm model to chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 환경 변수에서 설정 가져오기\n",
    "model_name = os.environ[\"MODEL_NAME\"]\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily\n",
    "Let's set up a tool called Tavily to allow our assistant to search the web when answering.  \n",
    "Go to [website](https://app.tavily.com/) and get api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# Tavily 검색 도구 설정 (최대 1개 결과)\n",
    "web_search_tool = TavilySearch(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Let's design a prompt for RAG that we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:  You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
      "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
      "You have provided with relevant background context to answer the question.\n",
      "\n",
      "Question: {question} \n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "print(\"Prompt Template: \", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Using LangGraph\n",
    "Let's define the State for our Graph. We'll track the user's question, our application's generation, and the list of relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    그래프의 상태를 나타냅니다.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    messages: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's define the nodes of our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def search(state):\n",
    "    \"\"\"\n",
    "    질문을 기반으로 웹 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        state (dict): 웹 검색 결과가 추가된 documents 키로 업데이트된 상태\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # 웹 검색 수행\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs[\"results\"]])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def explain(state: GraphState):\n",
    "    \"\"\"\n",
    "    컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): LLM 생성 결과가 포함된 messages 키가 추가된 상태\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    formatted = prompt.format(\n",
    "        question=question, \n",
    "        context=\"\\n\".join([d.page_content for d in documents])\n",
    "    )\n",
    "    generation = llm.invoke([HumanMessage(content=formatted)])\n",
    "    return {\"question\": question, \"messages\": [generation]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# 상태 그래프 생성\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# 노드 추가\n",
    "graph.add_node(\"explain\", explain)\n",
    "graph.add_node(\"search\", search)\n",
    "\n",
    "# 엣지 추가\n",
    "graph.add_edge(START, \"search\")\n",
    "graph.add_edge(\"search\", \"explain\")\n",
    "graph.add_edge(\"explain\", END)\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-based scorers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define a code-based scores, which are useful to measure deterministic or close-ended metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import scorer\n",
    "from mlflow.entities import Feedback\n",
    "\n",
    "@scorer\n",
    "def conciseness(outputs) -> Feedback:\n",
    "    words = outputs.split(\" \")\n",
    "    if len(words) <= 200:\n",
    "        return Feedback(\n",
    "            value=True,\n",
    "            rationale=\"Response is concise.\"\n",
    "        )\n",
    "    else:\n",
    "        return Feedback(\n",
    "            value=False,\n",
    "            rationale=\"Response is too long.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular custom code evaluator is a simple Python function that checks if our application produces outputs that are less than or equal to 200 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt-based judges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-ended metrics, it's can be powerful to use an LLM to score the outputs.\n",
    "\n",
    "Let's use an LLM to check whether our application produces correct outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to give an LLM our application's outputs, alongside the reference outputs stored in our dataset. \n",
    "\n",
    "The LLM will then be able to reference the \"right\" output to judge if our application's answer meets our accuracy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import custom_prompt_judge\n",
    "from mlflow.genai.scorers import scorer\n",
    "import mlflow\n",
    "\n",
    "correctness_rubric = custom_prompt_judge(\n",
    "    name=\"correctness\",\n",
    "    prompt_template=\"\"\"\n",
    "You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
    "\n",
    "<Instructions>\n",
    "    - Carefully read the input and output\n",
    "    - Use the reference output to determine if the model output contains errors\n",
    "    - Focus whether the model output uses accurate analogies and is logically consistent\n",
    "</Instructions>\n",
    "\n",
    "<Reminder>\n",
    "    The analogies in the output do not need to match the reference output exactly. Focus on logical consistency.\n",
    "</Reminder>\n",
    "\n",
    "<input>\n",
    "    {{input}}\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "    {{output}}\n",
    "</output>\n",
    "\n",
    "Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "<expected_response>\n",
    "    {{expected_response}}\n",
    "</expected_response>\n",
    "\n",
    "<Rubric>\n",
    "[[comprehensive]]: Identifies all issues, including edge cases, security concerns, and performance implications, and suggests specific improvements with examples.\n",
    "[[thorough]]: Catches major issues and most minor ones, provides good suggestions but may miss some edge cases.\n",
    "[[adequate]]: Identifies obvious issues and provides basic feedback, but misses subtle problems.\n",
    "[[superficial]]: Only catches surface-level issues, feedback is vague or generic.\n",
    "[[inadequate]]: Misses critical issues or provides incorrect feedback.\n",
    "</Rubric>\n",
    "\n",
    "\"\"\",\n",
    "    numeric_values={\n",
    "        \"comprehensive\": 1.0,\n",
    "        \"thorough\": 0.8,\n",
    "        \"adequate\": 0.6,\n",
    "        \"superficial\": 0.3,\n",
    "        \"inadequate\": 0.0,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "@scorer\n",
    "def correctness(inputs, outputs, expectations):\n",
    "    return correctness_rubric(\n",
    "        input=inputs[\"question\"],\n",
    "        output=outputs,\n",
    "        expected_response=expectations[\"expected_response\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the necessary components, so let's run our experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define evaluation dataset\n",
    "import mlflow.genai.datasets\n",
    "\n",
    "\n",
    "catalog_name = \"main\"  # replace with your catalog name\n",
    "schema_name = \"default\"  # replace with your schema name\n",
    "dataset_name = \"dataset_exmaple\"\n",
    "\n",
    "# Load existing dataset\n",
    "dataset = mlflow.genai.datasets.get_dataset(\n",
    "    uc_table_name=f\"{catalog_name}.{schema_name}.{dataset_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define predict_fn\n",
    "# predict_fn will be called for every row in your evaluation\n",
    "# dataset. Replace with your app's prediction function.  \n",
    "# NOTE: The **kwargs to predict_fn are the same as the keys of \n",
    "# the `inputs` in your dataset. \n",
    "def predict(question):\n",
    "  response = app.invoke({\"question\": question})\n",
    "  return response[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 14:56:40 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset.\n",
      "Evaluating: 100%|██████████| 3/3 [Elapsed: 00:15, Remaining: 00:00] , Time breakdown=(81.51% predict_fn, 18.49% scorers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results can be viewed from the MLflow run page.\n",
      "To compare evaluation results across runs, view the \"Evaluations\" tab of the experiment.\n",
      "\n",
      "Get aggregate metrics: `result.metrics`.\n",
      "Get per-row evaluation results: `result.tables['eval_results']`.\n",
      "`result` is the `EvaluationResult` object returned by `mlflow.evaluate`.\n",
      "\n",
      "🏃 View run nimble-snake-203 at: https://dbc-a3ca0892-0f44.cloud.databricks.com/ml/experiments/3243034829598956/runs/2d44609ba8164a77849b4fb7a876cf50\n",
      "🧪 View experiment at: https://dbc-a3ca0892-0f44.cloud.databricks.com/ml/experiments/3243034829598956\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Step 3: Run evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=dataset,\n",
    "    predict_fn=predict,\n",
    "    scorers=[conciseness, correctness]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-monitoring-3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
