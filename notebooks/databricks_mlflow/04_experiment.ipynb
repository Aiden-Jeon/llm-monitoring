{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "Setup llm model to chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ÌôòÍ≤Ω Î≥ÄÏàòÏóêÏÑú ÏÑ§Ï†ï Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "model_name = os.environ[\"MODEL_NAME\"]\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# LLM Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "llm = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily\n",
    "Let's set up a tool called Tavily to allow our assistant to search the web when answering.  \n",
    "Go to [website](https://app.tavily.com/) and get api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# Tavily Í≤ÄÏÉâ ÎèÑÍµ¨ ÏÑ§Ï†ï (ÏµúÎåÄ 1Í∞ú Í≤∞Í≥º)\n",
    "web_search_tool = TavilySearch(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Let's design a prompt for RAG that we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:  You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
      "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
      "You have provided with relevant background context to answer the question.\n",
      "\n",
      "Question: {question} \n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "print(\"Prompt Template: \", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Using LangGraph\n",
    "Let's define the State for our Graph. We'll track the user's question, our application's generation, and the list of relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Í∑∏ÎûòÌîÑÏùò ÏÉÅÌÉúÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    messages: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's define the nodes of our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def search(state):\n",
    "    \"\"\"\n",
    "    ÏßàÎ¨∏ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ïõπ Í≤ÄÏÉâÏùÑ ÏàòÌñâÌï©ÎãàÎã§.\n",
    "\n",
    "    Args:\n",
    "        state (dict): ÌòÑÏû¨ Í∑∏ÎûòÌîÑ ÏÉÅÌÉú\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Ïõπ Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä Ï∂îÍ∞ÄÎêú documents ÌÇ§Î°ú ÏóÖÎç∞Ïù¥Ìä∏Îêú ÏÉÅÌÉú\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # Ïõπ Í≤ÄÏÉâ ÏàòÌñâ\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs[\"results\"]])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def explain(state: GraphState):\n",
    "    \"\"\"\n",
    "    Ïª®ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú ÏùëÎãµÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): ÌòÑÏû¨ Í∑∏ÎûòÌîÑ ÏÉÅÌÉú\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): LLM ÏÉùÏÑ± Í≤∞Í≥ºÍ∞Ä Ìè¨Ìï®Îêú messages ÌÇ§Í∞Ä Ï∂îÍ∞ÄÎêú ÏÉÅÌÉú\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    formatted = prompt.format(\n",
    "        question=question, \n",
    "        context=\"\\n\".join([d.page_content for d in documents])\n",
    "    )\n",
    "    generation = llm.invoke([HumanMessage(content=formatted)])\n",
    "    return {\"question\": question, \"messages\": [generation]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ÏÉÅÌÉú Í∑∏ÎûòÌîÑ ÏÉùÏÑ±\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# ÎÖ∏Îìú Ï∂îÍ∞Ä\n",
    "graph.add_node(\"explain\", explain)\n",
    "graph.add_node(\"search\", search)\n",
    "\n",
    "# Ïó£ÏßÄ Ï∂îÍ∞Ä\n",
    "graph.add_edge(START, \"search\")\n",
    "graph.add_edge(\"search\", \"explain\")\n",
    "graph.add_edge(\"explain\", END)\n",
    "\n",
    "# Í∑∏ÎûòÌîÑ Ïª¥ÌååÏùº\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-based scorers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define a code-based scores, which are useful to measure deterministic or close-ended metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import scorer\n",
    "from mlflow.entities import Feedback\n",
    "\n",
    "@scorer\n",
    "def conciseness(outputs) -> Feedback:\n",
    "    words = outputs.split(\" \")\n",
    "    if len(words) <= 200:\n",
    "        return Feedback(\n",
    "            value=True,\n",
    "            rationale=\"Response is concise.\"\n",
    "        )\n",
    "    else:\n",
    "        return Feedback(\n",
    "            value=False,\n",
    "            rationale=\"Response is too long.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular custom code evaluator is a simple Python function that checks if our application produces outputs that are less than or equal to 200 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt-based judges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-ended metrics, it's can be powerful to use an LLM to score the outputs.\n",
    "\n",
    "Let's use an LLM to check whether our application produces correct outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to give an LLM our application's outputs, alongside the reference outputs stored in our dataset. \n",
    "\n",
    "The LLM will then be able to reference the \"right\" output to judge if our application's answer meets our accuracy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import custom_prompt_judge\n",
    "from mlflow.genai.scorers import scorer\n",
    "import mlflow\n",
    "\n",
    "correctness_rubric = custom_prompt_judge(\n",
    "    name=\"correctness\",\n",
    "    prompt_template=\"\"\"\n",
    "You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
    "\n",
    "<Instructions>\n",
    "    - Carefully read the input and output\n",
    "    - Use the reference output to determine if the model output contains errors\n",
    "    - Focus whether the model output uses accurate analogies and is logically consistent\n",
    "</Instructions>\n",
    "\n",
    "<Reminder>\n",
    "    The analogies in the output do not need to match the reference output exactly. Focus on logical consistency.\n",
    "</Reminder>\n",
    "\n",
    "<input>\n",
    "    {{input}}\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "    {{output}}\n",
    "</output>\n",
    "\n",
    "Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "<expected_response>\n",
    "    {{expected_response}}\n",
    "</expected_response>\n",
    "\n",
    "<Rubric>\n",
    "[[comprehensive]]: Identifies all issues, including edge cases, security concerns, and performance implications, and suggests specific improvements with examples.\n",
    "[[thorough]]: Catches major issues and most minor ones, provides good suggestions but may miss some edge cases.\n",
    "[[adequate]]: Identifies obvious issues and provides basic feedback, but misses subtle problems.\n",
    "[[superficial]]: Only catches surface-level issues, feedback is vague or generic.\n",
    "[[inadequate]]: Misses critical issues or provides incorrect feedback.\n",
    "</Rubric>\n",
    "\n",
    "\"\"\",\n",
    "    numeric_values={\n",
    "        \"comprehensive\": 1.0,\n",
    "        \"thorough\": 0.8,\n",
    "        \"adequate\": 0.6,\n",
    "        \"superficial\": 0.3,\n",
    "        \"inadequate\": 0.0,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "@scorer\n",
    "def correctness(inputs, outputs, expectations):\n",
    "    return correctness_rubric(\n",
    "        input=inputs[\"question\"],\n",
    "        output=outputs,\n",
    "        expected_response=expectations[\"expected_response\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the necessary components, so let's run our experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define evaluation dataset\n",
    "import mlflow.genai.datasets\n",
    "\n",
    "\n",
    "catalog_name = \"main\"  # replace with your catalog name\n",
    "schema_name = \"default\"  # replace with your schema name\n",
    "dataset_name = \"dataset_exmaple\"\n",
    "\n",
    "# Load existing dataset\n",
    "dataset = mlflow.genai.datasets.get_dataset(\n",
    "    uc_table_name=f\"{catalog_name}.{schema_name}.{dataset_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define predict_fn\n",
    "# predict_fn will be called for every row in your evaluation\n",
    "# dataset. Replace with your app's prediction function.  \n",
    "# NOTE: The **kwargs to predict_fn are the same as the keys of \n",
    "# the `inputs` in your dataset. \n",
    "def predict(question):\n",
    "  response = app.invoke({\"question\": question})\n",
    "  return response[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 14:56:40 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [Elapsed: 00:15, Remaining: 00:00] , Time breakdown=(81.51% predict_fn, 18.49% scorers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results can be viewed from the MLflow run page.\n",
      "To compare evaluation results across runs, view the \"Evaluations\" tab of the experiment.\n",
      "\n",
      "Get aggregate metrics: `result.metrics`.\n",
      "Get per-row evaluation results: `result.tables['eval_results']`.\n",
      "`result` is the `EvaluationResult` object returned by `mlflow.evaluate`.\n",
      "\n",
      "üèÉ View run nimble-snake-203 at: https://dbc-a3ca0892-0f44.cloud.databricks.com/ml/experiments/3243034829598956/runs/2d44609ba8164a77849b4fb7a876cf50\n",
      "üß™ View experiment at: https://dbc-a3ca0892-0f44.cloud.databricks.com/ml/experiments/3243034829598956\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Step 3: Run evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=dataset,\n",
    "    predict_fn=predict,\n",
    "    scorers=[conciseness, correctness]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-monitoring-3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
