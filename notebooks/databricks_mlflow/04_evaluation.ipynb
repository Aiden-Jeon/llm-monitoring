{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# ÌôòÍ≤Ω Î≥ÄÏàòÏóêÏÑú ÏÑ§Ï†ï Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "model_name = os.environ[\"MODEL_NAME\"]\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# LLM Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "llm = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    ")\n",
    "\n",
    "# Tavily Í≤ÄÏÉâ ÎèÑÍµ¨ ÏÑ§Ï†ï (ÏµúÎåÄ 1Í∞ú Í≤∞Í≥º)\n",
    "web_search_tool = TavilySearch(max_results=1)\n",
    "\n",
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Í∑∏ÎûòÌîÑÏùò ÏÉÅÌÉúÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    messages: List[str]\n",
    "\n",
    "\n",
    "def search(state):\n",
    "    \"\"\"\n",
    "    ÏßàÎ¨∏ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ïõπ Í≤ÄÏÉâÏùÑ ÏàòÌñâÌï©ÎãàÎã§.\n",
    "\n",
    "    Args:\n",
    "        state (dict): ÌòÑÏû¨ Í∑∏ÎûòÌîÑ ÏÉÅÌÉú\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Ïõπ Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä Ï∂îÍ∞ÄÎêú documents ÌÇ§Î°ú ÏóÖÎç∞Ïù¥Ìä∏Îêú ÏÉÅÌÉú\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # Ïõπ Í≤ÄÏÉâ ÏàòÌñâ\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs[\"results\"]])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def explain(state: GraphState):\n",
    "    \"\"\"\n",
    "    Ïª®ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú ÏùëÎãµÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "\n",
    "    Args:\n",
    "        state (dict): ÌòÑÏû¨ Í∑∏ÎûòÌîÑ ÏÉÅÌÉú\n",
    "\n",
    "    Returns:\n",
    "        state (dict): LLM ÏÉùÏÑ± Í≤∞Í≥ºÍ∞Ä Ìè¨Ìï®Îêú messages ÌÇ§Í∞Ä Ï∂îÍ∞ÄÎêú ÏÉÅÌÉú\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    formatted = prompt.format(\n",
    "        question=question, context=\"\\n\".join([d.page_content for d in documents])\n",
    "    )\n",
    "    generation = llm.invoke([HumanMessage(content=formatted)])\n",
    "    return {\"question\": question, \"messages\": [generation]}\n",
    "\n",
    "\n",
    "# ÏÉÅÌÉú Í∑∏ÎûòÌîÑ ÏÉùÏÑ±\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# ÎÖ∏Îìú Ï∂îÍ∞Ä\n",
    "graph.add_node(\"explain\", explain)\n",
    "graph.add_node(\"search\", search)\n",
    "\n",
    "# Ïó£ÏßÄ Ï∂îÍ∞Ä\n",
    "graph.add_edge(START, \"search\")\n",
    "graph.add_edge(\"search\", \"explain\")\n",
    "graph.add_edge(\"explain\", END)\n",
    "\n",
    "# Í∑∏ÎûòÌîÑ Ïª¥ÌååÏùº\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-based scorers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define a code-based scores, which are useful to measure deterministic or close-ended metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import scorer\n",
    "from mlflow.entities import Feedback\n",
    "\n",
    "@scorer\n",
    "def conciseness(outputs) -> Feedback:\n",
    "    words = outputs.split(\" \")\n",
    "    if len(words) <= 200:\n",
    "        return Feedback(\n",
    "            value=True,\n",
    "            rationale=\"Response is concise.\"\n",
    "        )\n",
    "    else:\n",
    "        return Feedback(\n",
    "            value=False,\n",
    "            rationale=\"Response is too long.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular custom code evaluator is a simple Python function that checks if our application produces outputs that are less than or equal to 200 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt-based judges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-ended metrics, it's can be powerful to use an LLM to score the outputs.\n",
    "\n",
    "Let's use an LLM to check whether our application produces correct outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to give an LLM our application's outputs, alongside the reference outputs stored in our dataset. \n",
    "\n",
    "The LLM will then be able to reference the \"right\" output to judge if our application's answer meets our accuracy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jongseob.jeon/.pyenv/versions/3.11.11/envs/llm-monitoring-3.11.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlflow.genai.judges import custom_prompt_judge\n",
    "from mlflow.genai.scorers import scorer\n",
    "import mlflow\n",
    "\n",
    "correctness_rubric = custom_prompt_judge(\n",
    "    name=\"correctness\",\n",
    "    prompt_template=\"\"\"\n",
    "You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
    "\n",
    "<Instructions>\n",
    "    - Carefully read the input and output\n",
    "    - Use the reference output to determine if the model output contains errors\n",
    "    - Focus whether the model output uses accurate analogies and is logically consistent\n",
    "</Instructions>\n",
    "\n",
    "<Reminder>\n",
    "    The analogies in the output do not need to match the reference output exactly. Focus on logical consistency.\n",
    "</Reminder>\n",
    "\n",
    "<input>\n",
    "    {{input}}\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "    {{output}}\n",
    "</output>\n",
    "\n",
    "Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "<expected_response>\n",
    "    {{expected_response}}\n",
    "</expected_response>\n",
    "\n",
    "<Rubric>\n",
    "[[comprehensive]]: Identifies all issues, including edge cases, security concerns, and performance implications, and suggests specific improvements with examples.\n",
    "[[thorough]]: Catches major issues and most minor ones, provides good suggestions but may miss some edge cases.\n",
    "[[adequate]]: Identifies obvious issues and provides basic feedback, but misses subtle problems.\n",
    "[[superficial]]: Only catches surface-level issues, feedback is vague or generic.\n",
    "[[inadequate]]: Misses critical issues or provides incorrect feedback.\n",
    "</Rubric>\n",
    "\n",
    "\"\"\",\n",
    "    numeric_values={\n",
    "        \"comprehensive\": 1.0,\n",
    "        \"thorough\": 0.8,\n",
    "        \"adequate\": 0.6,\n",
    "        \"superficial\": 0.3,\n",
    "        \"inadequate\": 0.0,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "@scorer\n",
    "def correctness(inputs, outputs, expectations):\n",
    "    return correctness_rubric(\n",
    "        input=inputs[\"question\"],\n",
    "        output=outputs,\n",
    "        expected_response=expectations[\"expected_response\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the necessary components, so let's run our experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define evaluation dataset\n",
    "import mlflow.genai.datasets\n",
    "\n",
    "\n",
    "catalog_name = \"main\"  # replace with your catalog name\n",
    "schema_name = \"default\"  # replace with your schema name\n",
    "dataset_name = \"dataset_exmaple\"\n",
    "\n",
    "# Load existing dataset\n",
    "dataset = mlflow.genai.datasets.get_dataset(\n",
    "    uc_table_name=f\"{catalog_name}.{schema_name}.{dataset_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define predict_fn\n",
    "# predict_fn will be called for every row in your evaluation\n",
    "# dataset. Replace with your app's prediction function.  \n",
    "# NOTE: The **kwargs to predict_fn are the same as the keys of \n",
    "# the `inputs` in your dataset. \n",
    "def predict(question):\n",
    "  response = app.invoke({\"question\": question})\n",
    "  return response[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/01 16:35:51 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset.\n",
      "2025/08/01 16:36:01 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [Elapsed: 00:14, Remaining: 00:00] , Time breakdown=(80.42% predict_fn, 19.58% scorers)\n",
      "/Users/jongseob.jeon/.pyenv/versions/3.11.11/envs/llm-monitoring-3.11.11/lib/python3.11/site-packages/databricks/rag_eval/mlflow/databricks_rag_evaluator.py:200: UserWarning: Some scorers failed during evaluation: correctness. Please check the evaluation result page for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results can be viewed from the MLflow run page.\n",
      "To compare evaluation results across runs, view the \"Evaluations\" tab of the experiment.\n",
      "\n",
      "Get aggregate metrics: `result.metrics`.\n",
      "Get per-row evaluation results: `result.tables['eval_results']`.\n",
      "`result` is the `EvaluationResult` object returned by `mlflow.evaluate`.\n",
      "\n",
      "üèÉ View run respected-hound-43 at: https://dbc-a3ca0892-0f44.cloud.databricks.com/ml/experiments/3243034829598956/runs/677d8878fdb241d39e561c75934dff38\n",
      "üß™ View experiment at: https://dbc-a3ca0892-0f44.cloud.databricks.com/ml/experiments/3243034829598956\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Step 3: Run evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=dataset,\n",
    "    predict_fn=predict,\n",
    "    scorers=[conciseness, correctness]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>trace</th>\n",
       "      <th>client_request_id</th>\n",
       "      <th>state</th>\n",
       "      <th>request_time</th>\n",
       "      <th>execution_duration</th>\n",
       "      <th>request</th>\n",
       "      <th>response</th>\n",
       "      <th>trace_metadata</th>\n",
       "      <th>tags</th>\n",
       "      <th>spans</th>\n",
       "      <th>assessments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr-8de37c44732eb587294d2a57f378804e</td>\n",
       "      <td>{'data': {'intermediate_outputs': {'LangGraph'...</td>\n",
       "      <td>tr-8de37c44732eb587294d2a57f378804e</td>\n",
       "      <td>OK</td>\n",
       "      <td>2025-08-01 07:36:01.465</td>\n",
       "      <td>4803</td>\n",
       "      <td>{'question': 'What is sound?'}</td>\n",
       "      <td>Oh boy, are you ready for a fun explanation?\\n...</td>\n",
       "      <td>{'mlflow.source.git.branch': 'main', 'mlflow.s...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'dbfs:/databricks/...</td>\n",
       "      <td>[{'trace_id': 'jeN8RHMutYcpTSpX83iATg==', 'spa...</td>\n",
       "      <td>[{'assessment_id': 'a-5a2153125d2a475ea6141077...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr-f33a73267cf64c8362df522605c0df3c</td>\n",
       "      <td>{'data': {'intermediate_outputs': {'LangGraph'...</td>\n",
       "      <td>tr-f33a73267cf64c8362df522605c0df3c</td>\n",
       "      <td>OK</td>\n",
       "      <td>2025-08-01 07:36:01.463</td>\n",
       "      <td>5087</td>\n",
       "      <td>{'question': 'How does a democracy work?'}</td>\n",
       "      <td>Let's talk about democracy.\\n\\nImagine you're ...</td>\n",
       "      <td>{'mlflow.source.git.branch': 'main', 'mlflow.s...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'dbfs:/databricks/...</td>\n",
       "      <td>[{'trace_id': '8zpzJnz2TINi31ImBcDfPA==', 'spa...</td>\n",
       "      <td>[{'assessment_id': 'a-983b381a03ce48f8a49651da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr-8338ce5d1767d09fff3f1b7ad74a8edd</td>\n",
       "      <td>{'data': {'intermediate_outputs': {'LangGraph'...</td>\n",
       "      <td>tr-8338ce5d1767d09fff3f1b7ad74a8edd</td>\n",
       "      <td>OK</td>\n",
       "      <td>2025-08-01 07:36:01.462</td>\n",
       "      <td>5178</td>\n",
       "      <td>{'question': 'How does string theory work?'}</td>\n",
       "      <td>Oh boy, are you ready for an adventure? Imagin...</td>\n",
       "      <td>{'mlflow.source.git.branch': 'main', 'mlflow.s...</td>\n",
       "      <td>{'mlflow.artifactLocation': 'dbfs:/databricks/...</td>\n",
       "      <td>[{'trace_id': 'gzjOXRdn0J//Pxt610qO3Q==', 'spa...</td>\n",
       "      <td>[{'assessment_id': 'a-b03098fe8d29498c9ca29227...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              trace_id  \\\n",
       "0  tr-8de37c44732eb587294d2a57f378804e   \n",
       "1  tr-f33a73267cf64c8362df522605c0df3c   \n",
       "2  tr-8338ce5d1767d09fff3f1b7ad74a8edd   \n",
       "\n",
       "                                               trace  \\\n",
       "0  {'data': {'intermediate_outputs': {'LangGraph'...   \n",
       "1  {'data': {'intermediate_outputs': {'LangGraph'...   \n",
       "2  {'data': {'intermediate_outputs': {'LangGraph'...   \n",
       "\n",
       "                     client_request_id state            request_time  \\\n",
       "0  tr-8de37c44732eb587294d2a57f378804e    OK 2025-08-01 07:36:01.465   \n",
       "1  tr-f33a73267cf64c8362df522605c0df3c    OK 2025-08-01 07:36:01.463   \n",
       "2  tr-8338ce5d1767d09fff3f1b7ad74a8edd    OK 2025-08-01 07:36:01.462   \n",
       "\n",
       "   execution_duration                                       request  \\\n",
       "0                4803                {'question': 'What is sound?'}   \n",
       "1                5087    {'question': 'How does a democracy work?'}   \n",
       "2                5178  {'question': 'How does string theory work?'}   \n",
       "\n",
       "                                            response  \\\n",
       "0  Oh boy, are you ready for a fun explanation?\\n...   \n",
       "1  Let's talk about democracy.\\n\\nImagine you're ...   \n",
       "2  Oh boy, are you ready for an adventure? Imagin...   \n",
       "\n",
       "                                      trace_metadata  \\\n",
       "0  {'mlflow.source.git.branch': 'main', 'mlflow.s...   \n",
       "1  {'mlflow.source.git.branch': 'main', 'mlflow.s...   \n",
       "2  {'mlflow.source.git.branch': 'main', 'mlflow.s...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  {'mlflow.artifactLocation': 'dbfs:/databricks/...   \n",
       "1  {'mlflow.artifactLocation': 'dbfs:/databricks/...   \n",
       "2  {'mlflow.artifactLocation': 'dbfs:/databricks/...   \n",
       "\n",
       "                                               spans  \\\n",
       "0  [{'trace_id': 'jeN8RHMutYcpTSpX83iATg==', 'spa...   \n",
       "1  [{'trace_id': '8zpzJnz2TINi31ImBcDfPA==', 'spa...   \n",
       "2  [{'trace_id': 'gzjOXRdn0J//Pxt610qO3Q==', 'spa...   \n",
       "\n",
       "                                         assessments  \n",
       "0  [{'assessment_id': 'a-5a2153125d2a475ea6141077...  \n",
       "1  [{'assessment_id': 'a-983b381a03ce48f8a49651da...  \n",
       "2  [{'assessment_id': 'a-b03098fe8d29498c9ca29227...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-monitoring-3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
