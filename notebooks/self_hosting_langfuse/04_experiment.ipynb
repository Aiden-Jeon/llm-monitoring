{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse client is authenticated and ready!\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "langfuse = get_client()\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# 연결 확인\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "Setup llm model to chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 환경 변수에서 설정 가져오기\n",
    "model_name = os.environ[\"MODEL_NAME\"]\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily\n",
    "Let's set up a tool called Tavily to allow our assistant to search the web when answering.  \n",
    "Go to [website](https://app.tavily.com/) and get api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# Tavily 검색 도구 설정 (최대 1개 결과)\n",
    "web_search_tool = TavilySearch(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Let's design a prompt for RAG that we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:  You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
      "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
      "You have provided with relevant background context to answer the question.\n",
      "\n",
      "Question: {question} \n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "print(\"Prompt Template: \", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Using LangGraph\n",
    "Let's define the State for our Graph. We'll track the user's question, our application's generation, and the list of relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    그래프의 상태를 나타냅니다.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    messages: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's define the nodes of our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def search(state):\n",
    "    \"\"\"\n",
    "    질문을 기반으로 웹 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        state (dict): 웹 검색 결과가 추가된 documents 키로 업데이트된 상태\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # 웹 검색 수행\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs[\"results\"]])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def explain(state: GraphState):\n",
    "    \"\"\"\n",
    "    컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): LLM 생성 결과가 포함된 messages 키가 추가된 상태\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    formatted = prompt.format(\n",
    "        question=question, \n",
    "        context=\"\\n\".join([d.page_content for d in documents])\n",
    "    )\n",
    "    generation = llm.invoke([HumanMessage(content=formatted)])\n",
    "    return {\"question\": question, \"messages\": [generation]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# 상태 그래프 생성\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# 노드 추가\n",
    "graph.add_node(\"explain\", explain)\n",
    "graph.add_node(\"search\", search)\n",
    "\n",
    "# 엣지 추가\n",
    "graph.add_edge(START, \"search\")\n",
    "graph.add_edge(\"search\", \"explain\")\n",
    "graph.add_edge(\"explain\", END)\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Code Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define a custom code evaluator, which are useful to measure deterministic or close-ended metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(outputs) -> bool:\n",
    "    words = outputs.split(\" \")\n",
    "    return len(words) <= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular custom code evaluator is a simple Python function that checks if our application produces outputs that are less than or equal to 200 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-ended metrics, it's can be powerful to use an LLM to score the outputs.\n",
    "\n",
    "Let's use an LLM to check whether our application produces correct outputs. First, let's define a scoring schema for our LLM to adhere to in its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define a scoring schema that our LLM must adhere to\n",
    "class CorrectnessScore(BaseModel):\n",
    "    \"\"\"Correctness score of the answer when compared to the reference answer.\"\"\"\n",
    "\n",
    "    score: int = Field(\n",
    "        description=\"The score of the correctness of the answer, from 0 to 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to give an LLM our application's outputs, alongside the reference outputs stored in our dataset. \n",
    "\n",
    "The LLM will then be able to reference the \"right\" output to judge if our application's answer meets our accuracy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def correctness(question, output, reference_output) -> bool:\n",
    "    prompt = \"\"\"\n",
    "    You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
    "\n",
    "    <Rubric>\n",
    "        A correct answer:\n",
    "        - Provides accurate information\n",
    "        - Uses suitable analogies and examples\n",
    "        - Contains no factual errors\n",
    "        - Is logically consistent\n",
    "\n",
    "        When scoring, you should penalize:\n",
    "        - Factual errors\n",
    "        - Incoherent analogies and examples\n",
    "        - Logical inconsistencies\n",
    "    </Rubric>\n",
    "\n",
    "    <Instructions>\n",
    "        - Carefully read the input and output\n",
    "        - Use the reference output to determine if the model output contains errors\n",
    "        - Focus whether the model output uses accurate analogies and is logically consistent\n",
    "    </Instructions>\n",
    "\n",
    "    <Reminder>\n",
    "        The analogies in the output do not need to match the reference output exactly. Focus on logical consistency.\n",
    "    </Reminder>\n",
    "\n",
    "    <input>\n",
    "        {}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "        {}\n",
    "    </output>\n",
    "\n",
    "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "    <reference_outputs>\n",
    "        {}\n",
    "    </reference_outputs>\n",
    "    \"\"\".format(\n",
    "        question, output, reference_output\n",
    "    )\n",
    "\n",
    "    structured_llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=openai_api_base,\n",
    "        temperature=0\n",
    "    ).with_structured_output(CorrectnessScore)\n",
    "    generation = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    return generation.score == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the necessary components, so let's run our experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define evaluation dataset\n",
    "dataset_name = \"dataset-exmaple\"\n",
    "dataset = langfuse.get_dataset(name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define run\n",
    "# Assume 'run' is your instrumented application function\n",
    "def run(question):\n",
    "    with langfuse.start_as_current_generation(name=\"qna-llm-call\") as generation:\n",
    "        # LLM call\n",
    "        response = app.invoke({\"question\": question})\n",
    "        output = response[\"messages\"][0].content\n",
    "        # Update the trace with the input and output\n",
    "        generation.update_trace(\n",
    "            input=question,\n",
    "            output=output,\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run evaluation\n",
    "from uuid import uuid4\n",
    "\n",
    "suffix = str(uuid4())[:8]\n",
    "current_run_name = f\"demo-run-{suffix}\"  # Identifies this specific evaluation run\n",
    "\n",
    "\n",
    "for item in dataset.items:\n",
    "    # Use the item.run() context manager\n",
    "    with item.run(\n",
    "        run_name=current_run_name, run_description=\"Evaluation run for tutorial\"\n",
    "    ) as root_span:  # root_span is the root span of the new trace for this item and run.\n",
    "        # All subsequent langfuse operations within this block are part of this trace.\n",
    "\n",
    "        # Call your application logic\n",
    "        question = item.input[\"question\"]\n",
    "        generated_answer = run(question=question)\n",
    "\n",
    "        # metrics\n",
    "        conciseness_score = conciseness(generated_answer)\n",
    "        root_span.score(\n",
    "            name=\"conciseness\",\n",
    "            value=conciseness_score,\n",
    "            data_type=\"NUMERIC\",\n",
    "        )\n",
    "\n",
    "        correctness_score = correctness(\n",
    "            question=question,\n",
    "            output=generated_answer,\n",
    "            reference_output=item.expected_output[\"text\"],\n",
    "        )\n",
    "        root_span.score(\n",
    "            name=\"correctness\",\n",
    "            value=correctness_score,\n",
    "            data_type=\"NUMERIC\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-monitoring-3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
