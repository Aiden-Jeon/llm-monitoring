{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "Setup llm model to chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 환경 변수에서 설정 가져오기\n",
    "model_name = os.environ[\"MODEL_NAME\"]\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily\n",
    "Let's set up a tool called Tavily to allow our assistant to search the web when answering.  \n",
    "Go to [website](https://app.tavily.com/) and get api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# Tavily 검색 도구 설정 (최대 1개 결과)\n",
    "web_search_tool = TavilySearch(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Let's design a prompt for RAG that we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:  You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
      "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
      "You have provided with relevant background context to answer the question.\n",
      "\n",
      "Question: {question} \n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "print(\"Prompt Template: \", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Using LangGraph\n",
    "Let's define the State for our Graph. We'll track the user's question, our application's generation, and the list of relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    그래프의 상태를 나타냅니다.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    messages: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's define the nodes of our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def search(state):\n",
    "    \"\"\"\n",
    "    질문을 기반으로 웹 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        state (dict): 웹 검색 결과가 추가된 documents 키로 업데이트된 상태\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # 웹 검색 수행\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs[\"results\"]])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def explain(state: GraphState):\n",
    "    \"\"\"\n",
    "    컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): LLM 생성 결과가 포함된 messages 키가 추가된 상태\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    formatted = prompt.format(\n",
    "        question=question, \n",
    "        context=\"\\n\".join([d.page_content for d in documents])\n",
    "    )\n",
    "    generation = llm.invoke([HumanMessage(content=formatted)])\n",
    "    return {\"question\": question, \"messages\": [generation]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# 상태 그래프 생성\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# 노드 추가\n",
    "graph.add_node(\"explain\", explain)\n",
    "graph.add_node(\"search\", search)\n",
    "\n",
    "# 엣지 추가\n",
    "graph.add_edge(START, \"search\")\n",
    "graph.add_edge(\"search\", \"explain\")\n",
    "graph.add_edge(\"explain\", END)\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Code Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define a custom code evaluator, which are useful to measure deterministic or close-ended metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(outputs: dict) -> bool:\n",
    "    words = outputs[\"output\"].split(\" \")\n",
    "    return len(words) <= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular custom code evaluator is a simple Python function that checks if our application produces outputs that are less than or equal to 200 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-ended metrics, it's can be powerful to use an LLM to score the outputs.\n",
    "\n",
    "Let's use an LLM to check whether our application produces correct outputs. First, let's define a scoring schema for our LLM to adhere to in its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define a scoring schema that our LLM must adhere to\n",
    "class CorrectnessScore(BaseModel):\n",
    "    \"\"\"Correctness score of the answer when compared to the reference answer.\"\"\"\n",
    "\n",
    "    score: int = Field(\n",
    "        description=\"The score of the correctness of the answer, from 0 to 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to give an LLM our application's outputs, alongside the reference outputs stored in our dataset. \n",
    "\n",
    "The LLM will then be able to reference the \"right\" output to judge if our application's answer meets our accuracy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    prompt = \"\"\"\n",
    "    You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
    "\n",
    "    <Rubric>\n",
    "        A correct answer:\n",
    "        - Provides accurate information\n",
    "        - Uses suitable analogies and examples\n",
    "        - Contains no factual errors\n",
    "        - Is logically consistent\n",
    "\n",
    "        When scoring, you should penalize:\n",
    "        - Factual errors\n",
    "        - Incoherent analogies and examples\n",
    "        - Logical inconsistencies\n",
    "    </Rubric>\n",
    "\n",
    "    <Instructions>\n",
    "        - Carefully read the input and output\n",
    "        - Use the reference output to determine if the model output contains errors\n",
    "        - Focus whether the model output uses accurate analogies and is logically consistent\n",
    "    </Instructions>\n",
    "\n",
    "    <Reminder>\n",
    "        The analogies in the output do not need to match the reference output exactly. Focus on logical consistency.\n",
    "    </Reminder>\n",
    "\n",
    "    <input>\n",
    "        {}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "        {}\n",
    "    </output>\n",
    "\n",
    "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "    <reference_outputs>\n",
    "        {}\n",
    "    </reference_outputs>\n",
    "    \"\"\".format(\n",
    "        inputs[\"question\"], outputs[\"output\"], reference_outputs[\"output\"]\n",
    "    )\n",
    "\n",
    "    structured_llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=openai_api_base,\n",
    "        temperature=0\n",
    "    ).with_structured_output(CorrectnessScore)\n",
    "    generation = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    return generation.score == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Run Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to run our application on the example inputs of our dataset. This is function that will be called when we run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a function to run your application\n",
    "def run(inputs: dict):\n",
    "    response = app.invoke({\"question\": inputs[\"question\"]})\n",
    "    return response[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the necessary components, so let's run our experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jongseob.jeon/.pyenv/versions/3.11.11/envs/llm-monitoring-3.11.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'demo-eval-82713c9e' at:\n",
      "https://smith.langchain.com/o/e68ba21c-3a02-4fb0-81cd-ae98057123fc/datasets/bc7c3a61-a249-47c3-8046-9fcbbed7e93f/compare?selectedSessions=2024e9c3-ea8d-4d5f-b85b-501010fc4939\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:19,  6.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.conciseness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does a democracy work?</td>\n",
       "      <td>So, you know how we live in a country with lot...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you and your friends want to dec...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>7.810723</td>\n",
       "      <td>483ac70d-a254-4181-bae9-f5cf6789b367</td>\n",
       "      <td>5c9fc102-26cf-4dc5-a197-4f7f3f512cca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is sound?</td>\n",
       "      <td>That's a great question about sound!\\n\\nSo, yo...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you have a drum. When you hit it...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4.953150</td>\n",
       "      <td>befbb020-f57b-46bb-8967-8a8f690dd0ed</td>\n",
       "      <td>04fd8cfc-bcaa-41c0-83e1-cd80475624df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does string theory work?</td>\n",
       "      <td>Let's talk about string theory!\\n\\nImagine you...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine that everything in the universe,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4.745685</td>\n",
       "      <td>f32b8859-a0b5-4702-9ef9-491cae6efa6f</td>\n",
       "      <td>09666029-5101-455a-8f2a-66cfb01d0ae3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults demo-eval-82713c9e>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "dataset_name = \"dataset-exmaple\"\n",
    "evaluate(\n",
    "    run,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, conciseness],\n",
    "    experiment_prefix=\"demo-eval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-monitoring-3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
